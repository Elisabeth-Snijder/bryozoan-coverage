{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427c8c1a",
   "metadata": {},
   "source": [
    "# Data retrieving of the bryozoan coverage on the kelp lamina\n",
    "**Author:** Jelte Doeksen\\\n",
    "**Co-Author:** Elisabeth Snijder \n",
    "\n",
    "![](https://i0.wp.com/dianaurban.com/wp-content/uploads/2017/10/cat-typing.gif?resize=400%2C400&ssl=1)\n",
    "\n",
    "Feature implementation wishlist:\n",
    "- Convert svg selections to pytorch rectangles for training models\n",
    "- Use Segment Anything Model to find more accurate pixel sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b66a41",
   "metadata": {},
   "source": [
    "\n",
    "## Install dependencies\n",
    "\n",
    "First make sure the dependencies are installed by running the following two commands in the terminal:\n",
    "\n",
    "<!-- `pip install git+https://github.com/facebookresearch/segment-anything.git` -->\n",
    "\n",
    "`pip install opencv-python matplotlib pandas openpyxl ipywidgets`\n",
    "\n",
    "<!-- Then download the [default SAM model](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and place it in the same folder as this notebook. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1d6bd",
   "metadata": {},
   "source": [
    "\n",
    "## Starting notebook\n",
    "\n",
    "Now, it's time to import the packages into your current Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from datetime import date\n",
    "from xml.dom import minidom\n",
    "from scipy import integrate\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778ad04",
   "metadata": {},
   "source": [
    "Setting the backround to transparant for nicer plot and widget outputs in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "   background-color: transparent !important;\n",
    "}\n",
    ".jp-OutputArea-output {\n",
    "   background-color: transparent;\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b94d2",
   "metadata": {},
   "source": [
    "## Preparing the inputs\n",
    "\n",
    "The next step is to prepare which pictures this script will run on, and to configure some specific settings from the manual selection work.\n",
    "\n",
    "The **`footage_dir`** is the path to the bryozoan scans in .tif format. This needs to be filled in.\n",
    "The `footage_list` is then generated by listing all the files for which also a scaled version ending with \" (kelp)\" and an SVG file are available. The list does not contain a file extension.\n",
    "From the `footage_list`, a `specimen_list` is extracted. This list contains the specimen id's, which consist of the first 4 characters of the file names, and represents the individual kelp leaves that were taken from the farm, before they were cut into scannable sections.\n",
    "\n",
    "The original dpi setting is entered in **`footage_dpi`**, to translate between pixels and millimeters.\n",
    "\n",
    "Next up is a list of dictionaries called **`feature_types`**, which holds the key information to distinguish between different types of biofouling features. Also a mapping is defined called **`svg_mapping`**, which will be used to translate the properties of the different SVG shapes into generic column values. Be aware that for example `size.x` may mean something different depending on the selected biofouling type.\n",
    "\n",
    "List **`sample_dates`** holds the dates on which specimens were collected from the farm.\n",
    "\n",
    "<!-- Finally, **`deployment_dates`** contains the dates on which the lines were deployed where the specimens originate from. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in footage_dir with the folder that contains the scans. Remember to use double backslashes on Windows\n",
    "footage_dir = os.environ[\"FOOTAGE_DIR\"]\n",
    "dir_separator = os.environ['DIR_SEPARATOR']\n",
    "svg_list = [ file.name.removesuffix('.svg') for file in os.scandir(footage_dir) if file.is_file() and file.name.endswith('.svg')]\n",
    "raw_footage_list = [ file.name.removesuffix('.tif') for file in os.scandir(footage_dir) if file.is_file() and file.path.endswith('.tif') and not file.path.endswith(').tif')]\n",
    "footage_list = set(svg_list) & set(raw_footage_list)\n",
    "specimen_list = [] # set([scan_id[:4] for scan_id in footage_list]) # Easy option: just take first 4 characters\n",
    "for scan_id in footage_list: # Advanced option: take first two numbers, in case a new sampling campaign takes 10+ specimens per date and/or samples on 10+ dates.\n",
    "    scan_id_numbers = re.findall(r'\\d+', scan_id)\n",
    "    specimen_list.append(f\"{scan_id_numbers[0]}SP{scan_id_numbers[1]}\")\n",
    "specimen_list = set(specimen_list)\n",
    "\n",
    "# Original footage dpi setting\n",
    "footage_dpi = int(os.environ[\"FOOTAGE_DPI\"])\n",
    "\n",
    "# Define feature_types for recognizing area's from the SVG files\n",
    "feature_types = [\n",
    "    {\n",
    "        \"type\": \"undetermined\",\n",
    "        \"shape\": \"circle\",\n",
    "        \"color\": \"#ff0c0c\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"recruitment\",\n",
    "        \"shape\": \"ellipse\",\n",
    "        \"color\": \"#1200d8\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"colony_top\",\n",
    "        \"shape\": \"ellipse\",\n",
    "        \"color\": \"#000000\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"colony_bottom\",\n",
    "        \"shape\": \"ellipse\",\n",
    "        \"color\": \"#03ce00\",\n",
    "    },\n",
    "    #{\n",
    "        # \"type\": \"negative\",\n",
    "        # \"shape\": \"rectangle\",\n",
    "        # \"color\": \"#000000\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "# Map specific properties of SVG elements to generic column names, including formula's for calculating surface area\n",
    "svg_mapping = {\n",
    "    'circle': {\n",
    "        'location.x': 'cx',\n",
    "        'location.y': 'cy',\n",
    "        'size.x': 'r',\n",
    "        'size.y': 'r',\n",
    "        'area': lambda sx, sy: math.pi * pow(sx, 2),\n",
    "    },\n",
    "    'ellipse': {\n",
    "        'location.x': 'cx',\n",
    "        'location.y': 'cy',\n",
    "        'size.x': 'rx',\n",
    "        'size.y': 'ry',\n",
    "        'area': lambda sx, sy: math.pi * sx * sy,\n",
    "    },\n",
    "    # 'rectangle': {\n",
    "    #     'location.x': 'x',\n",
    "    #     'location.y': 'y',\n",
    "    #     'size.x': 'width',\n",
    "    #     'size.y': 'height',\n",
    "    #     'area': lambda sx, sy: sx * sy,\n",
    "    # },\n",
    "}\n",
    "\n",
    "# Define dates on which specimens have been collected\n",
    "sample_dates = [\n",
    "    date(2023,4,25),\n",
    "    date(2023,5,10),\n",
    "    date(2023,5,15),\n",
    "    date(2023,5,23),\n",
    "    date(2023,6,7),\n",
    "    date(2023,6,15),\n",
    "    date(2023,6,29),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e8e05",
   "metadata": {},
   "source": [
    "## Preparing the outputs\n",
    "\n",
    "Now the output dataframes are prepared. The iterators in the next sections will fill these dataframes, after which they can be exported to an excelsheet.\n",
    "\n",
    "The dataframes are:\n",
    "\n",
    "`features`, listing all the selected features on all scans for which there is also an SVG and resized image available:\n",
    "- scan_id\n",
    "- specimen_id\n",
    "- sample_date\n",
    "- type\n",
    "- path_name\n",
    "- shape\n",
    "- color\n",
    "- location.x\n",
    "- location.y\n",
    "- size.x\n",
    "- size.y\n",
    "- rotation\n",
    "- area.sq_pixels\n",
    "- area.sq_millimeter\n",
    "- area.sq_meter\n",
    "\n",
    "`scan_aggregates`, listing the statistics for each scan, such as total kelp area on that scan, percentage coverage and counts per feature type:\n",
    "- scan_id\n",
    "- specimen_id\n",
    "- single_scan\n",
    "- sample_date\n",
    "- counts.total\n",
    "- counts.undetermined\n",
    "- counts.recruitments\n",
    "- counts.colonies\n",
    "- colony_size.mean\n",
    "- colony_size.min\n",
    "- colony_size.max\n",
    "- colony_size.std\n",
    "- scan_area.sq_pixels\n",
    "- scan_area.sq_millimeter\n",
    "- scan_area.sq_meter\n",
    "- kelp_area.percentage_of_scan_area\n",
    "- kelp_area.sq_pixels\n",
    "- kelp_area.sq_millimeter\n",
    "- kelp_area.sq_meter\n",
    "- kelp_area.sq_meter_coverage\n",
    "- kelp_area.percentage_coverage\n",
    "- counts_per_sq_meter.total\n",
    "- counts_per_sq_meter.undetermined\n",
    "- counts_per_sq_meter.recruitments\n",
    "- counts_per_sq_meter.colonies\n",
    "\n",
    "`specimen_aggregates`, a summary of scan aggregates aggregating all scans of a specimen:\n",
    "- specimen_id\n",
    "- sample_date\n",
    "- counts.total\n",
    "- counts.undetermined\n",
    "- counts.recruitments\n",
    "- counts.colonies\n",
    "- colony_size.mean\n",
    "- colony_size.min\n",
    "- colony_size.max\n",
    "- colony_size.std\n",
    "- kelp_area.sq_meter\n",
    "- kelp_area.sq_meter_coverage\n",
    "- kelp_area.percentage_coverage\n",
    "- counts_per_sq_meter.total\n",
    "- counts_per_sq_meter.undetermined\n",
    "- counts_per_sq_meter.recruitments\n",
    "- counts_per_sq_meter.colonies\n",
    "\n",
    "`event_aggregates`, aggregating one step higher, combining all information per date on which samples were collected from the farm, and adding statistics from the continuous measurements that were recorded in between these dates:\n",
    "- sample_date\n",
    "- number_of_specimens\n",
    "- number_of_scans\n",
    "- counts.total\n",
    "- counts.undetermined\n",
    "- counts.recruitments\n",
    "- counts.colonies\n",
    "- colony_size.mean\n",
    "- colony_size.min\n",
    "- colony_size.max\n",
    "- colony_size.std\n",
    "- kelp_area.sq_pixels_scaled\n",
    "- kelp_area.sq_pixels\n",
    "- kelp_area.sq_millimeter\n",
    "- kelp_area.sq_meter\n",
    "- kelp_area.sq_meter_coverage\n",
    "- kelp_area.percentage_coverage\n",
    "- counts_per_sq_meter.total\n",
    "- counts_per_sq_meter.undetermined\n",
    "- counts_per_sq_meter.recruitments\n",
    "- counts_per_sq_meter.colonies\n",
    "\n",
    "<!-- `raw_timeseries`, containing timeseries data from CTD and other sensors:\n",
    "- datetime\n",
    "- temperature\n",
    "- turbidity\n",
    "- chlorophyl -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5202e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframes\n",
    "features = pd.DataFrame()\n",
    "scan_aggregates = pd.DataFrame()\n",
    "specimen_aggregates = pd.DataFrame()\n",
    "event_aggregates = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab076da",
   "metadata": {},
   "source": [
    "## Iterating over selected footage to fill features, scan, specimen and event aggregates\n",
    "\n",
    "Now we start filling the dataframes. It starts with the simple ones by retrieving the contents of the SVG-files and aggregating that. In addition, OpenCV is used to transform the images into grayscale and a threshold is set to select what is kelp (the measurement tape is compensated for by deducting a fixed number of pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb92d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize progress bar\n",
    "progress = IntProgress(min=0, max=len(footage_list))\n",
    "display(progress)\n",
    "# First, iterate over all the SVG files and filter and add the contents to the features dataframe\n",
    "for scan_id in footage_list:\n",
    "    # Determine the sample date and specimen id of this scan\n",
    "    scan_id_numbers = re.findall(r'\\d+', scan_id)\n",
    "    sample_date = sample_dates[int(scan_id_numbers[0])-1]\n",
    "    specimen_id = f\"{scan_id_numbers[0]}SP{scan_id_numbers[1]}\"\n",
    "    \n",
    "    # Read SVG file\n",
    "    svg_doc = minidom.parse(f\"{footage_dir}{dir_separator}{scan_id}.svg\")\n",
    "    for feature_type in feature_types:\n",
    "        # For each feature, get the elements of corresponding shape and color\n",
    "        feature_elements = [element for element in svg_doc.getElementsByTagName(feature_type['shape']) if f\"stroke:{feature_type['color']};\" in element.getAttribute('style')]\n",
    "        for feature_element in feature_elements:\n",
    "            # Extract some values for which the SVG attribute depends on the shape\n",
    "            location_x = float(feature_element.getAttribute(svg_mapping[feature_type['shape']]['location.x']))\n",
    "            location_y = float(feature_element.getAttribute(svg_mapping[feature_type['shape']]['location.y']))\n",
    "            size_x = float(feature_element.getAttribute(svg_mapping[feature_type['shape']]['size.x']))\n",
    "            size_y = float(feature_element.getAttribute(svg_mapping[feature_type['shape']]['size.y']))\n",
    "            rotation = re.search('rotate\\((.*)\\)', feature_element.getAttribute('transform'))\n",
    "            area = svg_mapping[feature_type['shape']]['area'](size_x, size_y)\n",
    "            # For each element, add a samples entry\n",
    "            feaatures_entry = pd.DataFrame({\n",
    "                'scan_id': scan_id,\n",
    "                'specimen_id': specimen_id,\n",
    "                'sample_date': sample_date,\n",
    "                'type': feature_type['type'],\n",
    "                'path_name': feature_element.getAttribute('id'),\n",
    "                'shape': feature_type['shape'],\n",
    "                'color': feature_type['color'],\n",
    "                'location.x': location_x,\n",
    "                'location.y': location_y,\n",
    "                'size.x': size_x,\n",
    "                'size.y': size_y,\n",
    "                'rotation': float(rotation.group(1)) if rotation else 0.0,\n",
    "                'area.sq_pixels': round(area),\n",
    "            }, index=[0])\n",
    "            features = pd.concat([features, feaatures_entry], ignore_index=True)\n",
    "    svg_doc.unlink()\n",
    "    progress.value += 1\n",
    "\n",
    "# Now, add additional columns to the samples dataframe with derived values\n",
    "features['area.sq_millimeters'] = features['area.sq_pixels'] * pow(25.4, 2) / pow(footage_dpi, 2)\n",
    "features['area.sq_meters'] = features['area.sq_millimeters'] / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second step is to loop over all scans and fill the scan_aggregates dataframe with statistics about the features on them\n",
    "# Initialize progress bar\n",
    "progress = IntProgress(min=0, max=len(footage_list))\n",
    "display(progress)\n",
    "# This loop starts the same as in the previous step, but for readability it has been split up into two cells.\n",
    "for scan_id in footage_list:\n",
    "    # Determine the sample date and specimen id of this scan\n",
    "    scan_id_numbers = re.findall(r'\\d+', scan_id)\n",
    "    sample_date = sample_dates[int(scan_id_numbers[0])-1]\n",
    "    specimen_id = f\"{scan_id_numbers[0]}SP{scan_id_numbers[1]}\"\n",
    "\n",
    "    # Load image\n",
    "    scan = cv2.imread(f\"{footage_dir}{dir_separator}{scan_id}.tif\")\n",
    "    scan_gray = cv2.cvtColor(scan, cv2.COLOR_BGR2GRAY)\n",
    "    (threshold, mask) = cv2.threshold(scan_gray, 192, 255, cv2.THRESH_BINARY)\n",
    "    height, width, channels = scan.shape\n",
    "\n",
    "\n",
    "    # Plot and export the image for reference\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.gca()\n",
    "    ax.imshow(mask, cmap='gray')\n",
    "    fig.suptitle(f\"Threshold: {threshold}\", fontsize=18)\n",
    "    ax.axis('off')\n",
    "    fig.savefig(f\"{footage_dir}{dir_separator}{scan_id} (mask).png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Select the features on the scan and generate/fill aggregates\n",
    "    features_subset = features[features['scan_id'] == scan_id]\n",
    "    colony_subset = features_subset[(features_subset['type'] == 'colony_top') | (features_subset['type'] == 'colony_bottom')]\n",
    "    single_scan = sum([bool(re.match(scan_id[:-1], other_scan_id)) for other_scan_id in footage_list]) == 1\n",
    "    scan_aggregates_entry = pd.DataFrame({\n",
    "        'scan_id': scan_id,\n",
    "        'specimen_id': specimen_id,\n",
    "        'single_scan': single_scan,\n",
    "        'sample_date': sample_dates[int(re.findall(r'\\d+', scan_id)[0])-1], # Read the number from the scan ID\n",
    "        'counts.total': features_subset.shape[0],\n",
    "        'counts.undetermined': features_subset[features_subset['type'] == 'undetermined'].shape[0],\n",
    "        'counts.recruitments': features_subset[(features_subset['type'] == 'recruitment')].shape[0],\n",
    "        'counts.colonies': colony_subset.shape[0],\n",
    "        'colony_size.mean': colony_subset['area.sq_millimeters'].mean(),\n",
    "        'colony_size.min': colony_subset['area.sq_millimeters'].min(),\n",
    "        'colony_size.max': colony_subset['area.sq_millimeters'].max(),\n",
    "        'colony_size.std': colony_subset['area.sq_millimeters'].std(),\n",
    "        'scan_area.height_pixels': height,\n",
    "        'scan_area.width_pixels': width,\n",
    "        'kelp_area.sq_pixels': max(0, mask.astype(bool).sum()-6558000), # 6558000 is the approximate number of 'false-positive' pixels from the measurement tape for scans of the full length of the scan bed. Avoid going below zero\n",
    "        'kelp_area.sq_meter_coverage': features_subset['area.sq_meters'].sum(),\n",
    "    }, index=[0])\n",
    "    scan_aggregates = pd.concat([scan_aggregates, scan_aggregates_entry], ignore_index=True)\n",
    "    progress.value += 1\n",
    "\n",
    "\n",
    "# Again, add columns with derived information afterwards\n",
    "scan_aggregates['scan_area.sq_pixels'] = scan_aggregates['scan_area.height_pixels'] * scan_aggregates['scan_area.width_pixels']\n",
    "scan_aggregates['scan_area.sq_millimeter'] = scan_aggregates['scan_area.sq_pixels'] * pow(25.4, 2) / pow(footage_dpi, 2)\n",
    "scan_aggregates['scan_area.sq_meter'] = scan_aggregates['scan_area.sq_millimeter'] / 1000000\n",
    "scan_aggregates['kelp_area.percentage_of_scan_area'] = scan_aggregates['kelp_area.sq_pixels'] / scan_aggregates['scan_area.sq_pixels'] * 100\n",
    "scan_aggregates['kelp_area.sq_millimeter'] = (scan_aggregates['single_scan'] + 1) * scan_aggregates['kelp_area.sq_pixels'] * pow(25.4, 2) / pow(footage_dpi, 2) # (scan_aggregates['single_scan'] + 1) * doubles the kelp area for pieces that were analyzed from one side only\n",
    "scan_aggregates['kelp_area.sq_meter'] = scan_aggregates['kelp_area.sq_millimeter'] / 1000000\n",
    "scan_aggregates['kelp_area.percentage_coverage'] = scan_aggregates['kelp_area.sq_meter_coverage'] / scan_aggregates['kelp_area.sq_meter'] * 100\n",
    "scan_aggregates['counts_per_sq_meter.total'] = scan_aggregates['counts.total'] / scan_aggregates['kelp_area.sq_meter']\n",
    "scan_aggregates['counts_per_sq_meter.undetermined'] = scan_aggregates['counts.undetermined'] / scan_aggregates['kelp_area.sq_meter']\n",
    "scan_aggregates['counts_per_sq_meter.recruitments'] = scan_aggregates['counts.recruitments'] / scan_aggregates['kelp_area.sq_meter']\n",
    "scan_aggregates['counts_per_sq_meter.colonies'] = scan_aggregates['counts.colonies'] / scan_aggregates['kelp_area.sq_meter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4009630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a third step, the information from each scan is combined to receive the aggregates per specimen\n",
    "# Initialize progress bar\n",
    "progress = IntProgress(min=0, max=len(specimen_list))\n",
    "display(progress)\n",
    "# Loop over the specimen list\n",
    "for specimen_id in specimen_list:\n",
    "    # Select the subsets of scan aggregates and features from this specimen and generate/fill aggregates\n",
    "    # Use as much as possible the features dataframe, instead of having to calculate weighted avarages\n",
    "    features_subset = features[features['specimen_id'] == specimen_id]\n",
    "    colony_subset = features_subset[(features_subset['type'] == 'colony_top') | (features_subset['type'] == 'colony_bottom')]\n",
    "    scan_aggregates_subset = scan_aggregates[scan_aggregates['specimen_id'] == specimen_id]\n",
    "    specimen_aggregates_entry = pd.DataFrame({\n",
    "        'specimen_id': specimen_id,\n",
    "        'sample_date': sample_dates[int(re.findall(r'\\d+', specimen_id)[0])-1],\n",
    "        'number_of_scans': scan_aggregates_subset.shape[0],\n",
    "        'counts.total': features_subset.shape[0],\n",
    "        'counts.undetermined': features_subset[features_subset['type'] == 'undetermined'].shape[0],\n",
    "        'counts.recruitments': features_subset[(features_subset['type'] == 'recruitment')].shape[0],\n",
    "        'counts.colonies': colony_subset.shape[0],\n",
    "        'colony_size.mean': colony_subset['area.sq_millimeters'].mean(),\n",
    "        'colony_size.min': colony_subset['area.sq_millimeters'].min(),\n",
    "        'colony_size.max': colony_subset['area.sq_millimeters'].max(),\n",
    "        'colony_size.std': colony_subset['area.sq_millimeters'].std(),\n",
    "        'kelp_area.sq_meter': scan_aggregates_subset['kelp_area.sq_meter'].sum(),\n",
    "        'kelp_area.sq_meter_coverage': features_subset['area.sq_meters'].sum(),\n",
    "    }, index=[0])\n",
    "    specimen_aggregates = pd.concat([specimen_aggregates, specimen_aggregates_entry])\n",
    "    progress.value += 1\n",
    "\n",
    "# Again, add columns with derived information afterwards\n",
    "specimen_aggregates['kelp_area.percentage_coverage'] = 100 * specimen_aggregates['kelp_area.sq_meter_coverage'] / specimen_aggregates['kelp_area.sq_meter']\n",
    "specimen_aggregates['counts_per_sq_meter.total'] = specimen_aggregates['counts.total'] / specimen_aggregates['kelp_area.sq_meter']\n",
    "specimen_aggregates['counts_per_sq_meter.undetermined'] = specimen_aggregates['counts.undetermined'] / specimen_aggregates['kelp_area.sq_meter']\n",
    "specimen_aggregates['counts_per_sq_meter.recruitments'] = specimen_aggregates['counts.recruitments'] / specimen_aggregates['kelp_area.sq_meter']\n",
    "specimen_aggregates['counts_per_sq_meter.colonies'] = specimen_aggregates['counts.colonies'] / specimen_aggregates['kelp_area.sq_meter']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ef954",
   "metadata": {},
   "source": [
    "## Aggregation based on sampling events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate timeseries and per-sample aggregated data\n",
    "# Initialize progress bar\n",
    "progress = IntProgress(min=0, max=len(sample_dates))\n",
    "display(progress)\n",
    "# Loop over sample dates\n",
    "for sample_date in sample_dates:\n",
    "    # Select the subsets of specimen and scan aggregates and features from this sampling event and generate/fill aggregates\n",
    "    # Use as much as possible the features dataframe, instead of having to calculate weighted avarages\n",
    "    features_subset = features[features['sample_date'] == sample_date]\n",
    "    colony_subset = features_subset[(features_subset['type'] == 'colony_top') | (features_subset['type'] == 'colony_bottom')]\n",
    "    scan_aggregates_subset = scan_aggregates[scan_aggregates['sample_date'] == sample_date]\n",
    "    specimen_aggregates_subset = specimen_aggregates[specimen_aggregates['sample_date'] == sample_date]\n",
    "    event_aggregates_entry = pd.DataFrame({\n",
    "        'sample_date': sample_date,\n",
    "        'number_of_specimen': specimen_aggregates_subset.shape[0],\n",
    "        'number_of_scans': scan_aggregates_subset.shape[0],\n",
    "        'counts.total': features_subset.shape[0],\n",
    "        'counts.undetermined': features_subset[features_subset['type'] == 'undetermined'].shape[0],\n",
    "        'counts.recruitments': features_subset[(features_subset['type'] == 'recruitment')].shape[0],\n",
    "        'counts.colonies': colony_subset.shape[0],\n",
    "        'colony_size.mean': colony_subset['area.sq_millimeters'].mean(),\n",
    "        'colony_size.min': colony_subset['area.sq_millimeters'].min(),\n",
    "        'colony_size.max': colony_subset['area.sq_millimeters'].max(),\n",
    "        'colony_size.std': colony_subset['area.sq_millimeters'].std(),\n",
    "        'kelp_area.sq_meter': scan_aggregates_subset['kelp_area.sq_meter'].sum(),\n",
    "        'kelp_area.sq_meter_coverage': features_subset['area.sq_meters'].sum(),\n",
    "    }, index=[0])\n",
    "    event_aggregates = pd.concat([event_aggregates, event_aggregates_entry])\n",
    "    progress.value += 1\n",
    "\n",
    "# Again, add columns with derived information afterwards\n",
    "event_aggregates['kelp_area.percentage_coverage'] = 100 * event_aggregates['kelp_area.sq_meter_coverage'] / event_aggregates['kelp_area.sq_meter']\n",
    "event_aggregates['counts_per_sq_meter.total'] = event_aggregates['counts.total'] / event_aggregates['kelp_area.sq_meter']\n",
    "event_aggregates['counts_per_sq_meter.undetermined'] = event_aggregates['counts.undetermined'] / event_aggregates['kelp_area.sq_meter']\n",
    "event_aggregates['counts_per_sq_meter.recruitments'] = event_aggregates['counts.recruitments'] / event_aggregates['kelp_area.sq_meter']\n",
    "event_aggregates['counts_per_sq_meter.colonies'] = event_aggregates['counts.colonies'] / event_aggregates['kelp_area.sq_meter']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d892ab",
   "metadata": {},
   "source": [
    "## Export obtained dataframes to Excel\n",
    "\n",
    "The final step is to export all the tabular data to Excel for further processing. Each dataframe is exported to a separate sheet in the file, and the file is saved to the `footage_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(f\"{footage_dir}{dir_separator}bryozoan_coverage_quantification_results.xlsx\") as writer:\n",
    "    features.to_excel(writer, sheet_name='features', index=False)\n",
    "    scan_aggregates.to_excel(writer, sheet_name='scan_aggregates', index=False)\n",
    "    specimen_aggregates.to_excel(writer, sheet_name='specimen_aggregates', index=False)\n",
    "    event_aggregates.to_excel(writer, sheet_name='event_aggregates', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
